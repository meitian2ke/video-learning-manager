#!/bin/bash

# ç”Ÿäº§ç¯å¢ƒGPUéƒ¨ç½²è„šæœ¬
# é€‚ç”¨äºé…å¤‡NVIDIA GPUçš„DebianæœåŠ¡å™¨

set -e

echo "ğŸš€ å¼€å§‹éƒ¨ç½²è§†é¢‘å­¦ä¹ ç®¡ç†å™¨GPUç‰ˆæœ¬..."

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

print_status() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# 1. æ£€æŸ¥ç³»ç»Ÿè¦æ±‚
print_status "æ£€æŸ¥ç³»ç»Ÿè¦æ±‚..."

# æ£€æŸ¥Docker
if ! command -v docker &> /dev/null; then
    print_error "Dockeræœªå®‰è£…ï¼è¯·å…ˆå®‰è£…Docker"
    exit 1
fi

# æ£€æŸ¥Docker Compose
if ! command -v docker-compose &> /dev/null; then
    print_error "Docker Composeæœªå®‰è£…ï¼è¯·å…ˆå®‰è£…Docker Compose"
    exit 1
fi

# æ£€æŸ¥NVIDIAé©±åŠ¨
if ! command -v nvidia-smi &> /dev/null; then
    print_error "NVIDIAé©±åŠ¨æœªå®‰è£…ï¼è¯·å…ˆå®‰è£…NVIDIAé©±åŠ¨"
    exit 1
fi

# æ£€æŸ¥nvidia-container-toolkit
# æ£€æŸ¥nvidia-container-toolkit
if ! docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi >/dev/null 2>&1 && \
   ! docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi >/dev/null 2>&1; then
    print_error "nvidia-container-toolkitæœªæ­£ç¡®é…ç½®ï¼ï¼ˆnvidia-smi æ£€æµ‹å¤±è´¥ï¼‰"
    exit 1
fi

print_success "ç³»ç»Ÿè¦æ±‚æ£€æŸ¥é€šè¿‡ï¼"

# 2. æ£€æŸ¥GPUä¿¡æ¯
print_status "æ£€æŸ¥GPUä¿¡æ¯..."
nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader,nounits
print_success "GPUæ£€æŸ¥å®Œæˆ"

# 3. åˆ›å»ºå¿…è¦ç›®å½•
print_status "åˆ›å»ºæ•°æ®ç›®å½•..."
mkdir -p data/uploads data/videos data/audios data/thumbnails data/local-videos logs
chmod -R 755 data logs
print_success "ç›®å½•åˆ›å»ºå®Œæˆ"

# 4. æ£€æŸ¥ç«¯å£å ç”¨
print_status "æ£€æŸ¥ç«¯å£å ç”¨..."
if lsof -i:8000 &> /dev/null; then
    print_warning "ç«¯å£8000å·²è¢«å ç”¨"
    read -p "æ˜¯å¦å¼ºåˆ¶åœæ­¢å ç”¨ç«¯å£çš„è¿›ç¨‹ï¼Ÿ(y/N): " force_stop
    if [[ $force_stop =~ ^[Yy]$ ]]; then
        sudo lsof -ti:8000 | xargs sudo kill -9
        print_success "ç«¯å£å·²é‡Šæ”¾"
    else
        print_error "è¯·æ‰‹åŠ¨é‡Šæ”¾ç«¯å£8000åé‡æ–°è¿è¡Œ"
        exit 1
    fi
fi

# 5. åœæ­¢ç°æœ‰å®¹å™¨
print_status "åœæ­¢ç°æœ‰å®¹å™¨..."
docker-compose -f docker-compose.gpu.yml down --remove-orphans 2>/dev/null || true
print_success "ç°æœ‰å®¹å™¨å·²åœæ­¢"

# 6. æ„å»ºé•œåƒ
print_status "æ„å»ºGPUä¼˜åŒ–é•œåƒï¼ˆä½¿ç”¨å›½å†…é•œåƒæºåŠ é€Ÿï¼‰..."
docker-compose -f docker-compose.gpu.yml build --no-cache
print_success "é•œåƒæ„å»ºå®Œæˆ"

# 7. å¯åŠ¨æœåŠ¡
print_status "å¯åŠ¨GPUæœåŠ¡..."
docker-compose -f docker-compose.gpu.yml up -d
print_success "æœåŠ¡å¯åŠ¨å®Œæˆ"

# 8. ç­‰å¾…æœåŠ¡å°±ç»ª
print_status "ç­‰å¾…æœåŠ¡å¯åŠ¨..."
max_attempts=30
attempt=1

while [ $attempt -le $max_attempts ]; do
    if curl -s http://localhost:8000/health > /dev/null 2>&1; then
        print_success "æœåŠ¡å·²å°±ç»ªï¼"
        break
    fi
    
    if [ $attempt -eq $max_attempts ]; then
        print_error "æœåŠ¡å¯åŠ¨è¶…æ—¶ï¼"
        print_status "æŸ¥çœ‹å®¹å™¨æ—¥å¿—:"
        docker-compose -f docker-compose.gpu.yml logs --tail=50
        exit 1
    fi
    
    echo -n "."
    sleep 2
    ((attempt++))
done

# 9. æ˜¾ç¤ºéƒ¨ç½²ä¿¡æ¯
print_success "ğŸ‰ GPUç‰ˆæœ¬éƒ¨ç½²æˆåŠŸï¼"
echo ""
echo "ğŸ“Š æœåŠ¡ä¿¡æ¯:"
echo "  - å‰ç«¯åœ°å€: http://$(hostname -I | awk '{print $1}'):8000"
echo "  - APIæ–‡æ¡£: http://$(hostname -I | awk '{print $1}'):8000/docs"
echo "  - å¥åº·æ£€æŸ¥: http://$(hostname -I | awk '{print $1}'):8000/health"
echo "  - GPUç›‘æ§: http://$(hostname -I | awk '{print $1}'):9835 (å¦‚æœå¯ç”¨)"
echo ""
echo "ğŸ”§ ç®¡ç†å‘½ä»¤:"
echo "  - æŸ¥çœ‹æ—¥å¿—: docker-compose -f docker-compose.gpu.yml logs -f"
echo "  - åœæ­¢æœåŠ¡: docker-compose -f docker-compose.gpu.yml down"
echo "  - é‡å¯æœåŠ¡: docker-compose -f docker-compose.gpu.yml restart"
echo "  - æŸ¥çœ‹çŠ¶æ€: docker-compose -f docker-compose.gpu.yml ps"
echo ""
echo "ğŸ“ˆ æ€§èƒ½ç›‘æ§:"
echo "  - GPUä½¿ç”¨: nvidia-smi"
echo "  - å®¹å™¨çŠ¶æ€: docker stats video-learning-manager-gpu"
echo ""

# 10. æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ
print_status "æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ..."
sleep 5
nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu --format=csv,noheader,nounits

print_success "éƒ¨ç½²å®Œæˆï¼ç³»ç»Ÿå·²åˆ‡æ¢åˆ°GPUåŠ é€Ÿæ¨¡å¼ ğŸš€"