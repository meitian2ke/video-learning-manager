"""
ç³»ç»ŸçŠ¶æ€APIæ¥å£
æä¾›CPUã€GPUã€å†…å­˜ä½¿ç”¨æƒ…å†µå’Œè½¬å½•æ¨¡å¼ä¿¡æ¯
"""
from fastapi import APIRouter, HTTPException
from typing import Dict, Any
from app.utils.system_monitor import system_monitor
from app.services.ai_service import ai_service
from app.core.config import settings
import logging

logger = logging.getLogger(__name__)
router = APIRouter()

@router.get("/system/status")
async def get_system_status() -> Dict[str, Any]:
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    try:
        # è·å–ç³»ç»Ÿç›‘æ§ä¿¡æ¯
        status = system_monitor.get_system_status()
        can_transcribe, load_msg = system_monitor.is_suitable_for_transcription()
        
        # è·å–AIæœåŠ¡ä¿¡æ¯
        current_mode = getattr(ai_service, 'current_mode', None)
        environment = getattr(ai_service, 'environment', 'unknown')
        
        return {
            "timestamp": status["timestamp"],
            "system": {
                "cpu": status["cpu"],
                "memory": status["memory"],
                "gpu": status["gpu"],
                "load_status": status["load_status"],
                "can_transcribe": can_transcribe,
                "load_message": load_msg
            },
            "transcription": {
                "current_mode": "local",
                "configured_mode": settings.TRANSCRIPTION_MODE,
                "environment": environment,
                "gpu_available": status["gpu_available"],
                "force_cpu": settings.FORCE_CPU_MODE,
                "model": settings.WHISPER_MODEL,
                "device": "GPU" if status["gpu_available"] and not settings.FORCE_CPU_MODE else "CPU"
            },
            "configuration": {
                "max_concurrent": settings.MAX_CONCURRENT_TRANSCRIPTIONS,
                "available_slots": getattr(ai_service, 'transcription_semaphore', {}).get('_value', 'N/A'),
                "dev_cpu_limit": settings.DEV_CPU_LIMIT,
                "prod_cpu_limit": settings.PROD_CPU_LIMIT
            }
        }
    except Exception as e:
        logger.error(f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {str(e)}")

@router.post("/system/transcription-mode")
async def set_transcription_mode(mode: str) -> Dict[str, str]:
    """æ‰‹åŠ¨è®¾ç½®è½¬å½•æ¨¡å¼ï¼ˆä»…æ”¯æŒæœ¬åœ°æ¨¡å¼ï¼‰"""
    try:
        valid_modes = ["local", "auto"]
        if mode not in valid_modes:
            raise HTTPException(
                status_code=400, 
                detail=f"æ— æ•ˆçš„è½¬å½•æ¨¡å¼: {mode}. æœ‰æ•ˆé€‰é¡¹: {valid_modes}"
            )
        
        # æ›´æ–°é…ç½®ï¼ˆæ³¨æ„ï¼šè¿™åªæ˜¯ä¸´æ—¶æ›´æ–°ï¼Œé‡å¯åä¼šæ¢å¤ï¼‰
        settings.TRANSCRIPTION_MODE = mode
        
        logger.info(f"ğŸ“ è½¬å½•æ¨¡å¼å·²æ›´æ–°ä¸º: {mode}")
        
        return {
            "message": f"è½¬å½•æ¨¡å¼å·²è®¾ç½®ä¸º: {mode}",
            "previous_mode": getattr(ai_service, 'current_mode', 'local'),
            "new_mode": mode,
            "note": "ä»…æ”¯æŒæœ¬åœ°è½¬å½•æ¨¡å¼"
        }
    except Exception as e:
        logger.error(f"è®¾ç½®è½¬å½•æ¨¡å¼å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è®¾ç½®è½¬å½•æ¨¡å¼å¤±è´¥: {str(e)}")

@router.post("/system/force-cpu")
async def toggle_force_cpu(force: bool) -> Dict[str, Any]:
    """åˆ‡æ¢å¼ºåˆ¶CPUæ¨¡å¼"""
    try:
        settings.FORCE_CPU_MODE = force
        
        # å¦‚æœæœ‰å·²åŠ è½½çš„æ¨¡å‹ï¼Œéœ€è¦é‡æ–°åŠ è½½ä»¥åº”ç”¨æ–°è®¾ç½®
        if hasattr(ai_service, 'model') and ai_service.model is not None:
            logger.info("ğŸ”„ æ£€æµ‹åˆ°è®¾å¤‡æ¨¡å¼å˜æ›´ï¼Œå°†åœ¨ä¸‹æ¬¡è½¬å½•æ—¶é‡æ–°åŠ è½½æ¨¡å‹")
            ai_service.model = None  # é‡ç½®æ¨¡å‹ï¼Œä¸‹æ¬¡ä½¿ç”¨æ—¶ä¼šé‡æ–°åŠ è½½
        
        logger.info(f"ğŸ”§ å¼ºåˆ¶CPUæ¨¡å¼å·²{'å¯ç”¨' if force else 'ç¦ç”¨'}")
        
        return {
            "message": f"å¼ºåˆ¶CPUæ¨¡å¼å·²{'å¯ç”¨' if force else 'ç¦ç”¨'}",
            "force_cpu": force,
            "will_reload_model": True
        }
    except Exception as e:
        logger.error(f"åˆ‡æ¢å¼ºåˆ¶CPUæ¨¡å¼å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ‡æ¢å¼ºåˆ¶CPUæ¨¡å¼å¤±è´¥: {str(e)}")

@router.get("/system/performance-tips")
async def get_performance_tips() -> Dict[str, Any]:
    """è·å–æ€§èƒ½ä¼˜åŒ–å»ºè®®"""
    try:
        status = system_monitor.get_system_status()
        tips = []
        
        cpu_usage = status["cpu"]["usage_percent"]
        memory_percent = status["memory"]["percent"]
        gpu_available = status["gpu_available"]
        environment = getattr(ai_service, 'environment', 'unknown')
        
        # CPUè´Ÿè½½å»ºè®®
        if cpu_usage > 80:
            tips.append({
                "type": "warning",
                "title": "CPUè´Ÿè½½è¿‡é«˜",
                "message": f"å½“å‰CPUä½¿ç”¨ç‡ {cpu_usage:.1f}%ï¼Œå»ºè®®æš‚åœå…¶ä»–é«˜è´Ÿè½½ä»»åŠ¡",
                "action": "è€ƒè™‘åˆ‡æ¢åˆ°äº‘ç«¯è½¬å½•æˆ–é™ä½å¹¶å‘æ•°"
            })
        elif cpu_usage > 60:
            tips.append({
                "type": "info",
                "title": "CPUè´Ÿè½½è¾ƒé«˜",
                "message": f"å½“å‰CPUä½¿ç”¨ç‡ {cpu_usage:.1f}%ï¼Œç›‘æ§ç³»ç»Ÿæ€§èƒ½",
                "action": "å¦‚æœè½¬å½•ç¼“æ…¢å¯è€ƒè™‘äº‘ç«¯è½¬å½•"
            })
        
        # å†…å­˜å»ºè®®
        if memory_percent > 85:
            tips.append({
                "type": "warning",
                "title": "å†…å­˜ä¸è¶³",
                "message": f"å†…å­˜ä½¿ç”¨ç‡ {memory_percent:.1f}%ï¼Œå¯èƒ½å½±å“è½¬å½•æ€§èƒ½",
                "action": "å…³é—­ä¸å¿…è¦çš„åº”ç”¨ç¨‹åºæˆ–ä½¿ç”¨äº‘ç«¯è½¬å½•"
            })
        
        # GPUå»ºè®®
        if not gpu_available:
            tips.append({
                "type": "info",
                "title": "GPUæœªæ£€æµ‹åˆ°",
                "message": "å½“å‰ä½¿ç”¨CPUæ¨¡å¼ï¼Œè½¬å½•é€Ÿåº¦è¾ƒæ…¢",
                "action": "å¦‚æœ‰GPUå¯è€ƒè™‘å®‰è£…CUDAæ”¯æŒä»¥è·å¾—æ›´å¥½æ€§èƒ½"
            })
        elif gpu_available:
            tips.append({
                "type": "success",
                "title": "GPUå·²å¯ç”¨",
                "message": "æ£€æµ‹åˆ°GPUï¼Œè½¬å½•æ€§èƒ½å°†å¤§å¹…æå‡",
                "action": "å»ºè®®ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹(base/large)ä»¥è·å¾—æ›´å¥½å‡†ç¡®åº¦"
            })
        
        # ç¯å¢ƒå»ºè®®
        if environment == "development":
            tips.append({
                "type": "success",
                "title": "å¼€å‘ç¯å¢ƒä¼˜åŒ–",
                "message": "å·²å¯ç”¨å¼€å‘ç¯å¢ƒä¼˜åŒ–è®¾ç½®",
                "action": "å•ä»»åŠ¡æ¨¡å¼ï¼ŒCPUè´Ÿè½½é™åˆ¶70%"
            })
        elif environment == "production":
            tips.append({
                "type": "success",
                "title": "ç”Ÿäº§ç¯å¢ƒæ¨¡å¼",
                "message": "å·²æ£€æµ‹åˆ°GPUï¼Œå¯ç”¨ç”Ÿäº§ç¯å¢ƒæ¨¡å¼",
                "action": "å¯ä½¿ç”¨å¤šä»»åŠ¡å¹¶å‘å’ŒGPUåŠ é€Ÿ"
            })
        
        return {
            "environment": environment,
            "system_status": {
                "cpu_usage": cpu_usage,
                "memory_usage": memory_percent,
                "gpu_available": gpu_available
            },
            "tips": tips,
            "recommendations": {
                "best_mode": "local",
                "max_concurrent": 1 if environment == "development" else (3 if gpu_available else 1),
                "model_suggestion": "small" if not gpu_available else "base",
                "device_recommendation": "GPU" if gpu_available else "CPU"
            }
        }
    except Exception as e:
        logger.error(f"è·å–æ€§èƒ½å»ºè®®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ€§èƒ½å»ºè®®å¤±è´¥: {str(e)}")