version: '3.8'

services:
  # Whisperæ¨¡å‹åˆå§‹åŒ–æœåŠ¡
  whisper-models:
    build:
      context: .
      dockerfile: Dockerfile.models
    container_name: whisper-models-init
    volumes:
      - whisper-models:/root/.cache/huggingface
    environment:
      - HF_HOME=/root/.cache/huggingface
      - TRANSFORMERS_CACHE=/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: >
      python3 -c "
      from faster_whisper import WhisperModel;
      import os;
      print('ğŸ“¥ ä¸‹è½½Whisperæ¨¡å‹...');
      model = WhisperModel('base', device='cuda', compute_type='float16');
      print('âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ');"
    profiles:
      - init

  video-learning-manager-gpu:
    build:
      context: .
      dockerfile: ${DOCKERFILE:-Dockerfile.gpu}
    container_name: video-learning-manager-gpu
    ports:
      - "8000:8000"
    volumes:
      # æ•°æ®æŒä¹…åŒ–
      - ./data:/app/data
      - ./local-videos:/app/local-videos
      - ./logs:/app/logs
      # å…±äº«Whisperæ¨¡å‹ç¼“å­˜
      - whisper-models:/root/.cache/huggingface
      # GPUè®¿é—®
      - /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1:ro
      - /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:ro
    environment:
      # ç”Ÿäº§ç¯å¢ƒé…ç½®
      - ENVIRONMENT=production
      - WHISPER_MODEL=base
      - WHISPER_DEVICE=cuda
      - WHISPER_COMPUTE_TYPE=float16
      - MAX_CONCURRENT_TRANSCRIPTIONS=3
      - FORCE_CPU_MODE=false
      - AUTO_GPU_DETECTION=true
      
      # æ€§èƒ½ä¼˜åŒ–
      - WHISPER_NUM_WORKERS=1
      - WHISPER_THREADS=4
      - DEV_CPU_LIMIT=90.0
      - PROD_CPU_LIMIT=95.0
      
      # ç›®å½•é…ç½®
      - UPLOAD_DIR=/app/data/uploads
      - VIDEO_DIR=/app/data/videos
      - AUDIO_DIR=/app/data/audios
      - THUMBNAIL_DIR=/app/data/thumbnails
      - LOCAL_VIDEO_DIR=/app/local-videos
      
      # æ•°æ®åº“
      - DATABASE_URL=sqlite:///./data/video_learning.db
      
      # æ—¥å¿—çº§åˆ«
      - LOG_LEVEL=INFO
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    
    restart: unless-stopped
    
    # å¥åº·æ£€æŸ¥
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # å‰ç«¯æœåŠ¡
  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: video-learning-frontend
    ports:
      - "80:80"
    depends_on:
      - video-learning-manager-gpu
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # å¯é€‰ï¼šGPUæŒ‡æ ‡å¯¼å‡ºå™¨ï¼ˆç”¨äºç›‘æ§ï¼‰
  nvidia-smi-exporter:
    image: utkuozdemir/nvidia_gpu_exporter:1.2.0
    container_name: nvidia-smi-exporter
    ports:
      - "9835:9835"
    devices:
      - /dev/nvidiactl:/dev/nvidiactl
      - /dev/nvidia0:/dev/nvidia0
    volumes:
      - /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:ro
    restart: unless-stopped
    profiles:
      - monitoring

volumes:
  whisper-models:
    driver: local

networks:
  default:
    name: video-learning-network