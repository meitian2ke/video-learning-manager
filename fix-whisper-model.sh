#!/bin/bash

# ä¿®å¤Whisperæ¨¡å‹å’Œé…ç½®é—®é¢˜

set -e

echo "ğŸ”§ ä¿®å¤Whisperæ¨¡å‹å’ŒGPUé…ç½®..."

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

print_status() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# 1. åœæ­¢ç°æœ‰æœåŠ¡
print_status "åœæ­¢ç°æœ‰æœåŠ¡..."
docker-compose -f docker-compose.gpu.yml down

# 2. åˆ›å»ºæ¨¡å‹ä¸‹è½½è„šæœ¬
print_status "åˆ›å»ºWhisperæ¨¡å‹é¢„ä¸‹è½½è„šæœ¬..."
cat > download_whisper_model.py << 'EOF'
#!/usr/bin/env python3
import os
import torch
from faster_whisper import WhisperModel

def download_whisper_model():
    """é¢„ä¸‹è½½Whisperæ¨¡å‹"""
    print("ğŸ” æ£€æŸ¥GPUå¯ç”¨æ€§...")
    
    if torch.cuda.is_available():
        device = "cuda"
        compute_type = "float16"
        print(f"âœ… GPUå¯ç”¨: {torch.cuda.get_device_name()}")
    else:
        device = "cpu"
        compute_type = "int8"
        print("âš ï¸ GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨¡å¼")
    
    print(f"ğŸ“¥ ä¸‹è½½Whisper baseæ¨¡å‹åˆ° {device}...")
    
    try:
        # ä¸‹è½½æ¨¡å‹
        model = WhisperModel("base", device=device, compute_type=compute_type)
        print("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ!")
        
        # æµ‹è¯•æ¨¡å‹
        print("ğŸ§ª æµ‹è¯•æ¨¡å‹...")
        segments, info = model.transcribe("test", beam_size=5)
        print(f"âœ… æ¨¡å‹æµ‹è¯•æˆåŠŸ! è¯­è¨€: {info.language}, æ¦‚ç‡: {info.language_probability:.2f}")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
        return False
    
    return True

if __name__ == "__main__":
    success = download_whisper_model()
    exit(0 if success else 1)
EOF

# 3. æ›´æ–°Dockerfileé¢„ä¸‹è½½æ¨¡å‹
print_status "æ›´æ–°GPU Dockerfileæ·»åŠ æ¨¡å‹é¢„ä¸‹è½½..."

# å¤‡ä»½åŸæ–‡ä»¶
cp Dockerfile.gpu Dockerfile.gpu.backup

# åœ¨Dockerfileä¸­æ·»åŠ æ¨¡å‹ä¸‹è½½æ­¥éª¤
cat > Dockerfile.gpu << 'EOF'
# GPUä¼˜åŒ–ç‰ˆDockerfile - ä¸“ä¸ºç”Ÿäº§ç¯å¢ƒè®¾è®¡
FROM nvidia/cuda:11.8.0-devel-ubuntu22.04

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3.11-dev \
    python3-pip \
    ffmpeg \
    git \
    curl \
    wget \
    && rm -rf /var/lib/apt/lists/*

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å‡çº§pipï¼ˆç›´æ¥ä½¿ç”¨python3.11ï¼‰
RUN python3.11 -m pip install --upgrade pip

# å¤åˆ¶requirementsæ–‡ä»¶
COPY requirements.txt .

# é…ç½®pipä½¿ç”¨å›½å†…é•œåƒæºï¼ˆå¤§å¹…æå‡ä¸‹è½½é€Ÿåº¦ï¼‰
RUN python3.11 -m pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple/ && \
    python3.11 -m pip config set global.trusted-host pypi.tuna.tsinghua.edu.cn

# å®‰è£…PyTorch GPUç‰ˆæœ¬ï¼ˆä½¿ç”¨å›½å†…æº + CUDA 11.8ï¼‰
RUN python3.11 -m pip install torch torchvision torchaudio \
    -i https://pypi.tuna.tsinghua.edu.cn/simple/ \
    --extra-index-url https://download.pytorch.org/whl/cu118

# å®‰è£…å…¶ä»–ä¾èµ–
RUN python3.11 -m pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶æ¨¡å‹ä¸‹è½½è„šæœ¬
COPY download_whisper_model.py .

# é¢„ä¸‹è½½Whisperæ¨¡å‹
RUN python3.11 download_whisper_model.py

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY backend/ ./backend/
COPY local-videos/ ./local-videos/

# åˆ›å»ºå¿…è¦çš„ç›®å½•
RUN mkdir -p /app/data/uploads \
             /app/data/videos \
             /app/data/audios \
             /app/data/thumbnails \
             /app/data/local-videos \
             /app/logs

# è®¾ç½®æƒé™
RUN chmod -R 755 /app

# ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app/backend
ENV ENVIRONMENT=production
ENV WHISPER_DEVICE=cuda
ENV WHISPER_COMPUTE_TYPE=float16
ENV WHISPER_MODEL=base
ENV MAX_CONCURRENT_TRANSCRIPTIONS=3
ENV FORCE_CPU_MODE=false
ENV TRANSCRIPTION_MODE=local

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python3.11", "-m", "uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]
EOF

print_success "Dockerfileå·²æ›´æ–°ï¼Œæ·»åŠ äº†æ¨¡å‹é¢„ä¸‹è½½"

# 4. æ£€æŸ¥é…ç½®æ–‡ä»¶
print_status "æ£€æŸ¥åç«¯é…ç½®..."
if grep -q "TRANSCRIPTION_MODE.*openai" backend/app/core/config.py; then
    print_warning "å‘ç°é…ç½®ä¸­è¿˜æœ‰OpenAIæ¨¡å¼ï¼Œæ­£åœ¨ä¿®æ”¹ä¸ºæœ¬åœ°æ¨¡å¼..."
    
    # å¤‡ä»½é…ç½®æ–‡ä»¶
    cp backend/app/core/config.py backend/app/core/config.py.backup
    
    # ä¿®æ”¹é…ç½®ä¸ºæœ¬åœ°æ¨¡å¼
    sed -i 's/TRANSCRIPTION_MODE.*=.*"openai"/TRANSCRIPTION_MODE: str = "local"/' backend/app/core/config.py
    sed -i 's/TRANSCRIPTION_MODE.*=.*"auto"/TRANSCRIPTION_MODE: str = "local"/' backend/app/core/config.py
    
    print_success "é…ç½®å·²ä¿®æ”¹ä¸ºæœ¬åœ°æ¨¡å¼"
else
    print_success "é…ç½®å·²ç»æ˜¯æœ¬åœ°æ¨¡å¼"
fi

# 5. é‡æ–°æ„å»ºé•œåƒ
print_status "é‡æ–°æ„å»ºGPUé•œåƒï¼ˆåŒ…å«æ¨¡å‹é¢„ä¸‹è½½ï¼‰..."
docker-compose -f docker-compose.gpu.yml build --no-cache

# 6. å¯åŠ¨æœåŠ¡
print_status "å¯åŠ¨ä¿®å¤åçš„æœåŠ¡..."
docker-compose -f docker-compose.gpu.yml up -d

# 7. ç­‰å¾…æœåŠ¡å¯åŠ¨
print_status "ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 15

# 8. æ£€æŸ¥æœåŠ¡çŠ¶æ€
if curl -s http://localhost/health > /dev/null 2>&1; then
    print_success "âœ… æœåŠ¡å¯åŠ¨æˆåŠŸ!"
    echo ""
    echo "ğŸ“Š æœåŠ¡ä¿¡æ¯:"
    echo "  - å‰ç«¯ç•Œé¢: http://$(hostname -I | awk '{print $1}')"
    echo "  - GPUç›‘æ§: http://$(hostname -I | awk '{print $1}')/api/gpu/status"
    echo ""
    echo "ğŸ”§ æ•…éšœæ’é™¤:"
    echo "  - æŸ¥çœ‹æ—¥å¿—: docker-compose -f docker-compose.gpu.yml logs -f"
    echo "  - æ£€æŸ¥GPU: nvidia-smi"
    echo "  - æµ‹è¯•è½¬å½•: ä¸Šä¼ ä¸€ä¸ªå°è§†é¢‘æ–‡ä»¶"
else
    print_error "âŒ æœåŠ¡å¯åŠ¨å¤±è´¥!"
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥æ­¥éª¤:"
    echo "1. æŸ¥çœ‹å®¹å™¨çŠ¶æ€: docker-compose -f docker-compose.gpu.yml ps"
    echo "2. æŸ¥çœ‹åç«¯æ—¥å¿—: docker logs video-learning-manager-gpu"
    echo "3. æŸ¥çœ‹å‰ç«¯æ—¥å¿—: docker logs video-learning-frontend"
fi

print_success "ä¿®å¤è„šæœ¬æ‰§è¡Œå®Œæˆ!"