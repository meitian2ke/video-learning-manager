version: '3.8'

services:
  # Redis服务 - Celery消息队列
  redis:
    image: redis:7-alpine
    container_name: video-redis
    ports:
      - "6380:6379"
    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redis-data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  video-learning-manager-gpu:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    image: video-learning-manager-gpu:optimized
    container_name: video-learning-manager-gpu
    ports:
      - "8000:8000"
    volumes:
      # 数据持久化
      - ./data:/app/data
      - /data/videos/learning-manager:/app/local-videos
      - ./logs:/app/logs
      # 挂载本地模型目录
      - ./models/faster-whisper-large-v3:/root/.cache/huggingface/hub/models--Systran--faster-whisper-large-v3:ro
      # GPU访问
      - /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1:ro
      - /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:ro
    depends_on:
      - redis
    environment:
      # 生产环境配置
      - ENVIRONMENT=production
      - WHISPER_MODEL=large
      - WHISPER_DEVICE=cuda
      - WHISPER_COMPUTE_TYPE=float16
      - MAX_CONCURRENT_TRANSCRIPTIONS=3
      - TRANSCRIPTION_QUEUE_SIZE=50
      - FORCE_CPU_MODE=false
      - AUTO_GPU_DETECTION=true
      
      # 性能优化
      - WHISPER_NUM_WORKERS=1
      - WHISPER_THREADS=4
      - DEV_CPU_LIMIT=90.0
      - PROD_CPU_LIMIT=95.0
      
      # 目录配置
      - UPLOAD_DIR=/app/data/uploads
      - VIDEO_DIR=/app/data/videos
      - AUDIO_DIR=/app/data/audios
      - THUMBNAIL_DIR=/app/data/thumbnails
      - LOCAL_VIDEO_DIR=/app/local-videos
      
      # 数据库
      - DATABASE_URL=sqlite:///./data/video_learning.db
      
      # Redis配置
      - REDIS_URL=redis://redis:6379/0
      
      # 日志级别
      - LOG_LEVEL=INFO
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    
    restart: unless-stopped
    
    # 健康检查
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # 前端服务
  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: video-learning-frontend
    ports:
      - "3000:80"
    depends_on:
      - video-learning-manager-gpu
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # 可选：GPU指标导出器（用于监控）
  nvidia-smi-exporter:
    image: utkuozdemir/nvidia_gpu_exporter:1.2.0
    container_name: nvidia-smi-exporter
    ports:
      - "9835:9835"
    devices:
      - /dev/nvidiactl:/dev/nvidiactl
      - /dev/nvidia0:/dev/nvidia0
    volumes:
      - /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:ro
    restart: unless-stopped
    profiles:
      - monitoring

  # Whisper模型下载服务
  whisper-models:
    build:
      context: .
      dockerfile: Dockerfile.models
    container_name: whisper-models-downloader
    volumes:
      - whisper-models:/root/.cache/huggingface
    environment:
      - HF_HOME=/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles:
      - init

  # Celery Worker服务 - 视频处理队列
  celery-worker:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    image: video-learning-manager-gpu:optimized
    container_name: video-celery-worker
    command: celery -A app.celery_app worker --loglevel=info --concurrency=3 --max-tasks-per-child=10
    volumes:
      # 数据持久化
      - ./data:/app/data
      - /data/videos/learning-manager:/app/local-videos
      - ./logs:/app/logs
      # 挂载本地模型目录
      - ./models/faster-whisper-large-v3:/root/.cache/huggingface/hub/models--Systran--faster-whisper-large-v3:ro
      # GPU访问
      - /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1:ro
      - /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:ro
    depends_on:
      - redis
    environment:
      # 生产环境配置
      - ENVIRONMENT=production
      - WHISPER_MODEL=large
      - WHISPER_DEVICE=cuda
      - WHISPER_COMPUTE_TYPE=float16
      
      # 目录配置
      - UPLOAD_DIR=/app/data/uploads
      - VIDEO_DIR=/app/data/videos
      - AUDIO_DIR=/app/data/audios
      - THUMBNAIL_DIR=/app/data/thumbnails
      - LOCAL_VIDEO_DIR=/app/local-videos
      
      # 数据库
      - DATABASE_URL=sqlite:///./data/video_learning.db
      
      # Redis配置
      - REDIS_URL=redis://redis:6379/0
      
      # 日志级别
      - LOG_LEVEL=INFO
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "celery", "-A", "app.celery_app", "inspect", "ping"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 60s

volumes:
  whisper-models:
    driver: local
  redis-data:
    driver: local

networks:
  default:
    name: video-learning-network