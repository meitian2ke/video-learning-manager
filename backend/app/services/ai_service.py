import asyncio
import os
import re
import platform
from typing import Dict, Tuple, Optional, List
from pathlib import Path
import subprocess
import json
from faster_whisper import WhisperModel
from app.core.config import settings
from app.utils.system_monitor import system_monitor
import logging

# æœ¬åœ°è½¬å½•ä¸“ç”¨ï¼Œç§»é™¤ç¬¬ä¸‰æ–¹APIä¾èµ–

logger = logging.getLogger(__name__)

# åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘è½¬å½•æ•°é‡
transcription_semaphore = asyncio.Semaphore(settings.MAX_CONCURRENT_TRANSCRIPTIONS)

class AITranscriptionService:
    def __init__(self):
        self.model = None  # å•ä¸€æ¨¡å‹å®ä¾‹
        self.current_mode = None
        self.model_name = settings.WHISPER_MODEL  # ä½¿ç”¨é…ç½®çš„æ¨¡å‹
        self.environment = self._detect_environment()
        # ä¸åœ¨åˆå§‹åŒ–æ—¶åŠ è½½æ¨¡å‹ï¼Œé‡‡ç”¨æ‡’åŠ è½½æ¨¡å¼
        logger.info(f"ğŸ”§ AIè½¬å½•æœåŠ¡åˆå§‹åŒ– - ç¯å¢ƒ: {self.environment}, æ¨¡å‹: {self.model_name}")
    
    def _detect_environment(self) -> str:
        """æ£€æµ‹è¿è¡Œç¯å¢ƒ"""
        if settings.ENVIRONMENT != "auto":
            return settings.ENVIRONMENT
        
        # åŸºäºæ“ä½œç³»ç»Ÿå’Œç¡¬ä»¶è‡ªåŠ¨æ£€æµ‹
        system = platform.system().lower()
        if system == "darwin":  # macOS
            return "development"
        elif system == "linux":
            # æ£€æŸ¥æ˜¯å¦æœ‰GPU
            if system_monitor.gpu_available:
                return "production"
            else:
                return "development"
        else:
            return "development"
    
    def _choose_transcription_mode(self) -> str:
        """æ™ºèƒ½é€‰æ‹©è½¬å½•æ¨¡å¼ï¼ˆä»…æœ¬åœ°ï¼‰"""
        # æ£€æŸ¥ç³»ç»Ÿè´Ÿè½½
        can_transcribe, status_msg = system_monitor.is_suitable_for_transcription()
        
        if not can_transcribe:
            logger.warning(f"âš ï¸ ç³»ç»Ÿè´Ÿè½½è¿‡é«˜ï¼Œä½†ä»ä½¿ç”¨æœ¬åœ°è½¬å½•: {status_msg}")
        
        return "local"  # åªæ”¯æŒæœ¬åœ°è½¬å½•
    
    def _ensure_model_loaded(self):
        """ç¡®ä¿æ¨¡å‹å·²åŠ è½½ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if self.model is None:
            try:
                # æ™ºèƒ½é€‰æ‹©è®¾å¤‡å’Œè®¡ç®—ç±»å‹
                device = self._choose_device()
                compute_type = self._choose_compute_type()
                
                logger.info(f"ğŸ¤– æ­£åœ¨åŠ è½½Whisperæ¨¡å‹: {settings.WHISPER_MODEL}")
                logger.info(f"ğŸ¯ è®¾å¤‡: {device}, è®¡ç®—ç±»å‹: {compute_type}")
                
                # æ”¯æŒä¸¤ç§åŠ è½½æ–¹å¼ï¼šæ¨¡å‹åç§° æˆ– æœ¬åœ°è·¯å¾„
                model_path_or_name = self._get_model_path_or_name()
                
                self.model = WhisperModel(
                    model_path_or_name,
                    device=device,
                    compute_type=compute_type,
                    num_workers=getattr(settings, 'WHISPER_NUM_WORKERS', 1),
                    cpu_threads=getattr(settings, 'WHISPER_THREADS', 2)
                )
                logger.info(f"âœ… Whisperæ¨¡å‹ {settings.WHISPER_MODEL} åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ Whisperæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                raise Exception(f"æ¨¡å‹æœªå®‰è£…æˆ–æŸåï¼Œè¯·å…ˆæ‰‹åŠ¨ä¸‹è½½æ¨¡å‹: {e}")
        return self.model
    
    def _get_model_path_or_name(self) -> str:
        """è·å–æ¨¡å‹è·¯å¾„æˆ–åç§°"""
        # å¦‚æœæ˜¯largeæ¨¡å‹ï¼Œå°è¯•ä½¿ç”¨æœ¬åœ°è·¯å¾„åŠ è½½large-v3
        if settings.WHISPER_MODEL == "large":
            local_model_path = "/root/.cache/huggingface/hub/models--Systran--faster-whisper-large-v3"
            if os.path.exists(local_model_path):
                logger.info(f"ğŸ¯ ä½¿ç”¨æœ¬åœ°large-v3æ¨¡å‹: {local_model_path}")
                return local_model_path
            else:
                logger.info(f"ğŸ¯ æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ ‡å‡†largeæ¨¡å‹")
                return "large"
        
        # å…¶ä»–æƒ…å†µç›´æ¥ä½¿ç”¨æ¨¡å‹åç§°
        return settings.WHISPER_MODEL
    
    def _choose_device(self) -> str:
        """æ™ºèƒ½é€‰æ‹©è®¡ç®—è®¾å¤‡"""
        if settings.WHISPER_DEVICE != "auto":
            return settings.WHISPER_DEVICE
        
        if settings.FORCE_CPU_MODE:
            return "cpu"
        
        if self.environment == "production" and system_monitor.gpu_available:
            return "cuda"
        else:
            return "cpu"
    
    def _choose_compute_type(self) -> str:
        """æ™ºèƒ½é€‰æ‹©è®¡ç®—ç±»å‹"""
        if settings.WHISPER_COMPUTE_TYPE != "auto":
            return settings.WHISPER_COMPUTE_TYPE
        
        device = self._choose_device()
        
        if device == "cuda":
            # GPUç¯å¢ƒï¼Œä½¿ç”¨float16ä»¥è·å¾—æ›´å¥½æ€§èƒ½
            return "float16"
        else:
            # CPUç¯å¢ƒï¼Œä½¿ç”¨int8ä»¥èŠ‚çœå†…å­˜å’Œæé«˜é€Ÿåº¦
            return "int8"
    
    def download_model(self):
        """æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹"""
        try:
            logger.info(f"å¼€å§‹ä¸‹è½½Whisperæ¨¡å‹: {settings.WHISPER_MODEL}")
            # è¿™ä¼šè§¦å‘æ¨¡å‹ä¸‹è½½
            model = WhisperModel(
                settings.WHISPER_MODEL,
                device=settings.WHISPER_DEVICE,
                compute_type=settings.WHISPER_COMPUTE_TYPE,
                num_workers=getattr(settings, 'WHISPER_NUM_WORKERS', 1),
                cpu_threads=getattr(settings, 'WHISPER_THREADS', 2)
            )
            self.model = model
            logger.info("æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    async def download_video(self, url: str, video_id: int) -> Tuple[str, Dict]:
        """ä¸‹è½½è§†é¢‘å¹¶è·å–ä¿¡æ¯"""
        try:
            output_dir = Path(settings.VIDEO_DIR) / str(video_id)
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # yt-dlp å‘½ä»¤
            cmd = [
                "yt-dlp",
                "--extract-flat",  # åªè·å–ä¿¡æ¯ï¼Œä¸ä¸‹è½½
                "--dump-json",
                url
            ]
            
            # è·å–è§†é¢‘ä¿¡æ¯
            result = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            stdout, stderr = await result.communicate()
            
            if result.returncode != 0:
                raise Exception(f"è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {stderr.decode()}")
            
            video_info = json.loads(stdout.decode())
            
            # ä¸‹è½½è§†é¢‘
            download_cmd = [
                "yt-dlp",
                "--format", "best[height<=720]",  # é™åˆ¶ç”»è´¨ä»¥èŠ‚çœç©ºé—´
                "--output", str(output_dir / "%(title)s.%(ext)s"),
                "--write-thumbnail",
                url
            ]
            
            download_result = await asyncio.create_subprocess_exec(
                *download_cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            await download_result.communicate()
            
            if download_result.returncode != 0:
                raise Exception("è§†é¢‘ä¸‹è½½å¤±è´¥")
            
            # æŸ¥æ‰¾ä¸‹è½½çš„æ–‡ä»¶
            video_files = list(output_dir.glob("*"))
            video_file = next((f for f in video_files if f.suffix in ['.mp4', '.webm', '.mkv']), None)
            
            if not video_file:
                raise Exception("æœªæ‰¾åˆ°ä¸‹è½½çš„è§†é¢‘æ–‡ä»¶")
            
            return str(video_file), {
                "title": video_info.get("title", ""),
                "duration": video_info.get("duration", 0),
                "thumbnail": video_info.get("thumbnail", ""),
                "platform": self._detect_platform(url)
            }
            
        except Exception as e:
            logger.error(f"ä¸‹è½½è§†é¢‘å¤±è´¥: {e}")
            raise
    
    async def extract_audio(self, video_path: str) -> str:
        """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘"""
        try:
            video_file = Path(video_path)
            audio_path = Path(settings.AUDIO_DIR) / f"{video_file.stem}.wav"
            
            # ç¡®ä¿éŸ³é¢‘ç›®å½•å­˜åœ¨
            audio_path.parent.mkdir(parents=True, exist_ok=True)
            
            # FFmpeg æå–éŸ³é¢‘
            cmd = [
                "ffmpeg",
                "-i", str(video_file),
                "-ac", "1",  # å•å£°é“
                "-ar", "16000",  # 16kHzé‡‡æ ·ç‡
                "-y",  # è¦†ç›–å·²å­˜åœ¨æ–‡ä»¶
                str(audio_path)
            ]
            
            result = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            await result.communicate()
            
            if result.returncode != 0 or not audio_path.exists():
                raise Exception("éŸ³é¢‘æå–å¤±è´¥")
            
            return str(audio_path)
            
        except Exception as e:
            logger.error(f"æå–éŸ³é¢‘å¤±è´¥: {e}")
            raise
    
    async def transcribe_audio(self, audio_path: str) -> Dict:
        """è½¬å½•éŸ³é¢‘ä¸ºæ–‡å­—ï¼ˆå¸¦å¹¶å‘æ§åˆ¶ï¼‰"""
        async with transcription_semaphore:  # æ§åˆ¶å¹¶å‘æ•°é‡
            try:
                logger.info(f"å¼€å§‹è½¬å½•éŸ³é¢‘: {audio_path} (å½“å‰å¯ç”¨æ§½ä½: {transcription_semaphore._value})")
                
                if not self.model:
                    self._ensure_model_loaded()
                
                # æ‰§è¡Œè½¬å½•
                segments, info = self.model.transcribe(audio_path)
                
                # æ”¶é›†è½¬å½•ç»“æœ
                transcript_segments = []
                full_text = ""
                
                for segment in segments:
                    segment_data = {
                        "start": segment.start,
                        "end": segment.end,
                        "text": segment.text.strip()
                    }
                    transcript_segments.append(segment_data)
                    full_text += segment.text.strip() + " "
                
                # æ¸…ç†æ–‡æœ¬
                cleaned_text = self._clean_text(full_text)
                
                # ç”Ÿæˆæ‘˜è¦å’Œæ ‡ç­¾
                summary = self._generate_summary(cleaned_text)
                tags = self._extract_tags(cleaned_text)
                
                logger.info(f"è½¬å½•å®Œæˆï¼Œå…±è½¬å½• {len(transcript_segments)} ä¸ªç‰‡æ®µ (é‡Šæ”¾æ§½ä½)")
                
                return {
                    "original_text": full_text.strip(),
                    "cleaned_text": cleaned_text,
                    "summary": summary,
                    "tags": ", ".join(tags),
                    "language": info.language,
                    "confidence_score": info.language_probability,
                    "segments": transcript_segments
                }
                
            except Exception as e:
                logger.error(f"è½¬å½•éŸ³é¢‘å¤±è´¥: {e} (é‡Šæ”¾æ§½ä½)")
                raise
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ - å¢å¼ºç‰ˆæœ¬ï¼ŒæŒ‰å¥å·åˆ†è¡Œï¼Œæå‡å¯è¯»æ€§"""
        # å»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        
        # å»é™¤å¸¸è§çš„æ— æ„ä¹‰è¯æ±‡å’Œå¡«å……è¯
        noise_words = ['å—¯', 'å•Š', 'å‘ƒ', 'è¿™ä¸ª', 'é‚£ä¸ª', 'ç„¶å', 'å°±æ˜¯', 'æˆ‘ä»¬', 'ä½ ä»¬']
        for word in noise_words:
            text = text.replace(word, '')
        
        # æ™ºèƒ½æ–­å¥ - æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†è¡Œ
        text = re.sub(r'([ã€‚ï¼ï¼Ÿï¼›])\s*', r'\1\n', text)
        
        # å¤„ç†é€—å· - é€‚å½“æ·»åŠ æ¢è¡Œæå‡å¯è¯»æ€§
        text = re.sub(r'([ï¼Œ,])\s*([A-Z]|\d+|[ä¸€-é¾¯]{3,})', r'\1\n\2', text)
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œå’Œç©ºæ ¼
        text = re.sub(r'\n\s*\n', '\n', text)
        text = re.sub(r'^\s+|\s+$', '', text, flags=re.MULTILINE)
        
        return text.strip()
    
    def _generate_summary(self, text: str) -> str:
        """ç”Ÿæˆæ™ºèƒ½æ‘˜è¦"""
        sentences = text.replace('\n', '').split('ã€‚')
        
        # è¿‡æ»¤æœ‰æ•ˆå¥å­
        valid_sentences = [s.strip() for s in sentences if len(s.strip()) > 15]
        
        if not valid_sentences:
            return "æ— æ³•ç”Ÿæˆæ‘˜è¦"
        
        # æ™ºèƒ½é€‰æ‹©å…³é”®å¥å­ï¼ˆåŒ…å«æ•°å­—ã€å…¬å¸åã€äº§å“åçš„å¥å­ä¼˜å…ˆï¼‰
        important_keywords = ['ä¸‡', 'äº¿', 'å…ƒ', 'ç¾å…ƒ', 'å¼€å‘è€…', 'å¹³å°', 'æ¡†æ¶', 'å¼€æº', 'AI', 'API']
        scored_sentences = []
        
        for sentence in valid_sentences[:8]:  # æœ€å¤šåˆ†æå‰8å¥
            score = 0
            # åŒ…å«é‡è¦å…³é”®è¯çš„å¥å­å¾—åˆ†æ›´é«˜
            for keyword in important_keywords:
                if keyword in sentence:
                    score += 1
            # å¥å­é•¿åº¦é€‚ä¸­çš„å¾—åˆ†æ›´é«˜
            if 20 <= len(sentence) <= 80:
                score += 1
            scored_sentences.append((score, sentence))
        
        # æŒ‰å¾—åˆ†æ’åºï¼Œå–å‰3å¥
        scored_sentences.sort(key=lambda x: x[0], reverse=True)
        summary_sentences = [s[1] for s in scored_sentences[:3]]
        
        return 'ã€‚'.join(summary_sentences) + 'ã€‚'
    
    def _extract_tags(self, text: str) -> List[str]:
        """æ™ºèƒ½æå–å…³é”®è¯æ ‡ç­¾å’Œä¸»é¢˜åˆ†ç±»"""
        # æ‰©å±•çš„æŠ€æœ¯å’Œå•†ä¸šå…³é”®è¯åº“
        keyword_categories = {
            'AIæŠ€æœ¯': ['AI', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ', 'GPT', 'LLM', 'å¤§æ¨¡å‹'],
            'ç¼–ç¨‹å¼€å‘': ['Python', 'JavaScript', 'React', 'Vue', 'Node.js', 'Django', 'FastAPI', 
                      'ç¼–ç¨‹', 'å¼€å‘', 'å‰ç«¯', 'åç«¯', 'æ¡†æ¶', 'API', 'ç®—æ³•', 'æ•°æ®ç»“æ„'],
            'å·¥å…·å¹³å°': ['GitHub', 'Docker', 'Git', 'Linux', 'éƒ¨ç½²', 'æµ‹è¯•', 'CI/CD', 'äº‘æœåŠ¡'],
            'æ•°æ®çˆ¬è™«': ['çˆ¬è™«', 'æ•°æ®æŠ“å–', 'ç½‘é¡µ', 'æ•°æ®æ¸…æ´—', 'ç»“æ„åŒ–æ•°æ®', 'webæ•°æ®'],
            'å•†ä¸šæŠ•èµ„': ['èèµ„', 'æŠ•èµ„', 'ä¸‡ç¾å…ƒ', 'äº¿', 'åˆ›ä¸š', 'YC', 'Nexus', 'ä¼°å€¼'],
            'äº§å“æœåŠ¡': ['å¹³å°', 'æœåŠ¡', 'ç”¨æˆ·', 'å¼€å‘è€…', 'ä¼ä¸š', 'SaaS', 'å¼€æº']
        }
        
        tags = []
        text_clean = text.replace('\n', ' ').lower()
        
        # æŒ‰åˆ†ç±»æå–å…³é”®è¯
        for category, keywords in keyword_categories.items():
            found_in_category = []
            for keyword in keywords:
                if keyword.lower() in text_clean or keyword in text:
                    found_in_category.append(keyword)
            
            # å¦‚æœè¯¥åˆ†ç±»ä¸‹æœ‰å…³é”®è¯ï¼Œæ·»åŠ åˆ†ç±»æ ‡ç­¾
            if found_in_category:
                tags.append(category)
                # æ·»åŠ å…·ä½“çš„å…³é”®è¯ï¼ˆæœ€å¤š2ä¸ªï¼‰
                tags.extend(found_in_category[:2])
        
        # æ•°å­—ä¿¡æ¯æå–
        import re
        numbers = re.findall(r'\d+[ä¸‡äº¿]?[ç¾å…ƒå…ƒ]?', text)
        if numbers:
            tags.append('æ•°æ®æŒ‡æ ‡')
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_tags = list(dict.fromkeys(tags))  # ä¿æŒé¡ºåºå»é‡
        return unique_tags[:8]  # æœ€å¤šè¿”å›8ä¸ªæ ‡ç­¾
    
    def _generate_smart_title(self, text: str) -> str:
        """æ™ºèƒ½ç”Ÿæˆè§†é¢‘æ ‡é¢˜"""
        # æå–å…³é”®ä¿¡æ¯æ¥ç”Ÿæˆæ ‡é¢˜
        sentences = text.replace('\n', '').split('ã€‚')
        
        if not sentences:
            return "è§†é¢‘å†…å®¹æ‘˜è¦"
        
        first_sentence = sentences[0].strip()
        
        # æŸ¥æ‰¾å…¬å¸åã€äº§å“åã€æŠ€æœ¯åç­‰å…³é”®ä¿¡æ¯
        key_entities = []
        
        # å…¬å¸å’Œäº§å“åç§°
        companies = ['YC', 'Firecrawl', 'Nexus', 'GitHub', 'OpenAI', 'Google', 'Meta', 'Apple']
        for company in companies:
            if company in text:
                key_entities.append(company)
        
        # æŠ€æœ¯å…³é”®è¯
        tech_terms = ['AI', 'çˆ¬è™«', 'å¤§æ¨¡å‹', 'API', 'å¼€æº', 'å¹³å°', 'æ¡†æ¶']
        for term in tech_terms:
            if term in text:
                key_entities.append(term)
        
        # æ•°å­—ä¿¡æ¯
        import re
        numbers = re.findall(r'\d+[ä¸‡äº¿ç¾å…ƒ]+', text)
        if numbers:
            key_entities.extend(numbers[:2])  # æœ€å¤šæ·»åŠ 2ä¸ªæ•°å­—
        
        # ç”Ÿæˆæ ‡é¢˜
        if key_entities:
            # ä¼˜å…ˆä½¿ç”¨å‰30ä¸ªå­—ç¬¦ + å…³é”®å®ä½“
            base_title = first_sentence[:30]
            entities_str = " | ".join(key_entities[:3])  # æœ€å¤š3ä¸ªå…³é”®å®ä½“
            return f"{base_title} - {entities_str}"
        else:
            # å¦‚æœæ²¡æœ‰å…³é”®å®ä½“ï¼Œä½¿ç”¨å‰50ä¸ªå­—ç¬¦
            return first_sentence[:50] + ("..." if len(first_sentence) > 50 else "")
    
    def _calculate_importance_score(self, text: str, tags: List[str]) -> float:
        """è®¡ç®—é‡è¦æ€§è¯„åˆ† (1.0-5.0)"""
        score = 3.0  # åŸºç¡€åˆ†æ•°
        
        # åŸºäºå†…å®¹é•¿åº¦è¯„åˆ† (è¯¦ç»†çš„å†…å®¹é€šå¸¸æ›´é‡è¦)
        if len(text) > 1000:
            score += 0.5
        elif len(text) < 200:
            score -= 0.5
        
        # åŸºäºæ ‡ç­¾æ•°é‡å’Œè´¨é‡è¯„åˆ†
        high_value_tags = ['AIæŠ€æœ¯', 'å•†ä¸šæŠ•èµ„', 'æ•°æ®æŒ‡æ ‡']
        medium_value_tags = ['ç¼–ç¨‹å¼€å‘', 'å·¥å…·å¹³å°', 'äº§å“æœåŠ¡']
        
        for tag in tags:
            if tag in high_value_tags:
                score += 0.8
            elif tag in medium_value_tags:
                score += 0.4
        
        # åŸºäºå…³é”®è¯å¯†åº¦è¯„åˆ†
        important_keywords = ['ä¸‡ç¾å…ƒ', 'äº¿', 'èèµ„', 'å¼€æº', 'AI', 'å¹³å°', 'API']
        keyword_count = sum(1 for keyword in important_keywords if keyword in text)
        score += min(keyword_count * 0.2, 1.0)
        
        # åŸºäºæ•°å­—ä¿¡æ¯è¯„åˆ† (åŒ…å«å…·ä½“æ•°æ®çš„å†…å®¹æ›´æœ‰ä»·å€¼)
        import re
        numbers = re.findall(r'\d+[ä¸‡äº¿ç¾å…ƒå…ƒ]+', text)
        if numbers:
            score += 0.6
        
        # ç¡®ä¿åˆ†æ•°åœ¨1.0-5.0èŒƒå›´å†…
        return max(1.0, min(5.0, score))
    
    def _format_text_for_display(self, text: str) -> str:
        """æ ¼å¼åŒ–æ–‡æœ¬ç”¨äºæ˜¾ç¤ºï¼Œå¢å¼ºå¯è¯»æ€§"""
        # æŒ‰å¥å·åˆ†æ®µï¼Œæ¯å¥ä¸€è¡Œ
        sentences = text.split('ã€‚')
        formatted_lines = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # æ£€æµ‹é‡è¦ä¿¡æ¯å¹¶åŠ ç²—æ ‡è®°
            if self._is_important_sentence(sentence):
                formatted_lines.append(f"**{sentence}ã€‚**")
            else:
                formatted_lines.append(f"{sentence}ã€‚")
        
        return '\n'.join(formatted_lines)
    
    def _is_important_sentence(self, sentence: str) -> bool:
        """åˆ¤æ–­å¥å­æ˜¯å¦é‡è¦ï¼ˆç”¨äºåŠ ç²—æ˜¾ç¤ºï¼‰"""
        important_indicators = [
            r'\d+[ä¸‡äº¿][ç¾å…ƒå…ƒ]',  # é‡‘é¢æ•°å­—
            r'\d+ä¸‡[å¼€å‘è€…ç”¨æˆ·]',  # ç”¨æˆ·æ•°é‡
            r'å¼€æº|GitHub',  # å¼€æºç›¸å…³
            r'å¹³å°|æ¡†æ¶|API',  # æŠ€æœ¯å¹³å°
            r'YC|Nexus|é¢†[å¤´æŠ•]',  # æŠ•èµ„æœºæ„
        ]
        
        for pattern in important_indicators:
            if re.search(pattern, sentence):
                return True
        
        return False
    
    def _detect_platform(self, url: str) -> str:
        """æ£€æµ‹è§†é¢‘å¹³å°"""
        url_lower = url.lower()
        
        if 'douyin.com' in url_lower or 'tiktok.com' in url_lower:
            return 'douyin'
        elif 'weixin' in url_lower or 'mp.weixin.qq.com' in url_lower:
            return 'weixin'
        elif 'bilibili.com' in url_lower:
            return 'bilibili'
        elif 'youtube.com' in url_lower or 'youtu.be' in url_lower:
            return 'youtube'
        elif 'xiaohongshu.com' in url_lower:
            return 'xiaohongshu'
        else:
            return 'unknown'
    
    async def cleanup_files(self, video_id: int):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            video_dir = Path(settings.VIDEO_DIR) / str(video_id)
            audio_files = Path(settings.AUDIO_DIR).glob(f"{video_id}_*.wav")
            
            # åˆ é™¤éŸ³é¢‘æ–‡ä»¶ï¼ˆä¿ç•™è§†é¢‘æ–‡ä»¶ç”¨äºåç»­éœ€è¦ï¼‰
            for audio_file in audio_files:
                if audio_file.exists():
                    audio_file.unlink()
            
            logger.info(f"æ¸…ç†è§†é¢‘ {video_id} çš„ä¸´æ—¶æ–‡ä»¶å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
    
    async def transcribe_video(self, video_path: str) -> Dict:
        """æ™ºèƒ½è½¬å½•è§†é¢‘æ–‡ä»¶ï¼ˆå¸¦å¹¶å‘æ§åˆ¶å’Œè´Ÿè½½ç›‘æ§ï¼‰"""
        async with transcription_semaphore:  # æ§åˆ¶å¹¶å‘æ•°é‡
            try:
                logger.info(f"ğŸ¬ å¼€å§‹è½¬å½•è§†é¢‘: {os.path.basename(video_path)}")
                logger.info(f"ğŸ“Š å½“å‰å¯ç”¨æ§½ä½: {transcription_semaphore._value}")
                
                # è®°å½•ç³»ç»ŸçŠ¶æ€
                system_monitor.log_system_status()
                
                # æ™ºèƒ½é€‰æ‹©è½¬å½•æ¨¡å¼
                mode = self._choose_transcription_mode()
                self.current_mode = mode
                
                logger.info(f"ğŸ¤– é€‰æ‹©è½¬å½•æ¨¡å¼: {mode}")
                logger.info(f"ğŸ—ï¸ è¿è¡Œç¯å¢ƒ: {self.environment}")
                
                logger.info("ğŸ’» === ä½¿ç”¨æœ¬åœ°Whisperæ¨¡å‹è½¬å½• ===")
                result = await self._transcribe_with_local_model(video_path)
                logger.info("âœ… === æœ¬åœ°è½¬å½•å®Œæˆ ===")
                
                # è½¬å½•åå†æ¬¡è®°å½•ç³»ç»ŸçŠ¶æ€
                system_monitor.log_system_status()
                
                return result
                    
            except Exception as e:
                logger.error(f"âŒ è½¬å½•è§†é¢‘å¤±è´¥: {e} (é‡Šæ”¾æ§½ä½)")
                return {
                    "original_text": f"è½¬å½•å¤±è´¥: {str(e)}",
                    "cleaned_text": f"è½¬å½•å¤±è´¥: {str(e)}",
                    "formatted_text": f"è½¬å½•å¤±è´¥: {str(e)}",
                    "summary": "è§†é¢‘è½¬å½•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯",
                    "smart_title": "è½¬å½•å¤±è´¥",
                    "tags": "è½¬å½•å¤±è´¥",
                    "importance_score": 1.0,
                    "language": "zh",
                    "confidence_score": 0.0,
                    "segments": []
                }
    
    
    async def _transcribe_with_local_model(self, video_path: str) -> Dict:
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹è½¬å½•"""
        try:
            # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
            self._ensure_model_loaded()
            
            # faster-whisper å¯ä»¥ç›´æ¥å¤„ç†è§†é¢‘æ–‡ä»¶
            logger.info("æ­£åœ¨ä½¿ç”¨æœ¬åœ°Whisperæ¨¡å‹è½¬å½•è§†é¢‘...")
            segments, info = self.model.transcribe(
                video_path,
                language="zh",  # æŒ‡å®šä¸ºä¸­æ–‡
                task="transcribe"
            )
            
            # æ”¶é›†è½¬å½•ç»“æœ
            transcript_segments = []
            full_text = ""
            
            for segment in segments:
                segment_data = {
                    "start": segment.start,
                    "end": segment.end,
                    "text": segment.text.strip()
                }
                transcript_segments.append(segment_data)
                full_text += segment.text.strip() + " "
            
            if not full_text.strip():
                logger.warning("è½¬å½•ç»“æœä¸ºç©ºï¼Œå¯èƒ½æ˜¯è§†é¢‘æ²¡æœ‰éŸ³é¢‘æˆ–éŸ³é¢‘è´¨é‡é—®é¢˜")
                return {
                    "original_text": "æœªæ£€æµ‹åˆ°éŸ³é¢‘å†…å®¹",
                    "cleaned_text": "æœªæ£€æµ‹åˆ°éŸ³é¢‘å†…å®¹",
                    "summary": "è¯¥è§†é¢‘å¯èƒ½æ²¡æœ‰éŸ³é¢‘å†…å®¹æˆ–éŸ³é¢‘è´¨é‡è¾ƒå·®",
                    "tags": "æ— éŸ³é¢‘",
                    "language": "zh",
                    "confidence_score": 0.0,
                    "segments": []
                }
            
            # æ™ºèƒ½æ–‡æœ¬å¤„ç†å’Œåˆ†æ
            cleaned_text = self._clean_text(full_text)
            formatted_text = self._format_text_for_display(cleaned_text)
            summary = self._generate_summary(cleaned_text)
            tags = self._extract_tags(cleaned_text)
            smart_title = self._generate_smart_title(cleaned_text)
            importance_score = self._calculate_importance_score(cleaned_text, tags)
            
            logger.info(f"âœ… æœ¬åœ°è½¬å½•å®Œæˆï¼Œå…±è½¬å½• {len(transcript_segments)} ä¸ªç‰‡æ®µï¼Œé‡è¦æ€§è¯„åˆ†: {importance_score:.1f}")
            
            return {
                "original_text": full_text.strip(),
                "cleaned_text": cleaned_text,
                "formatted_text": formatted_text,
                "summary": summary,
                "smart_title": smart_title,
                "tags": ", ".join(tags),
                "importance_score": importance_score,
                "language": info.language,
                "confidence_score": info.language_probability,
                "segments": transcript_segments
            }
                
        except Exception as e:
            logger.error(f"è½¬å½•è§†é¢‘å¤±è´¥: {e} (é‡Šæ”¾æ§½ä½)")
            # è¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return {
                "original_text": f"è½¬å½•å¤±è´¥: {str(e)}",
                "cleaned_text": f"è½¬å½•å¤±è´¥: {str(e)}",
                "summary": "è§†é¢‘è½¬å½•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯",
                "tags": "è½¬å½•å¤±è´¥",
                "language": "zh",
                "confidence_score": 0.0,
                "segments": []
            }

# å…¨å±€æœåŠ¡å®ä¾‹
ai_service = AITranscriptionService()