# GPU优化版Dockerfile - 避免重复下载，解决版本冲突
FROM nvidia/cuda:11.8.0-devel-ubuntu22.04

# 设置环境变量
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3.11-dev \
    python3-pip \
    ffmpeg \
    git \
    curl \
    wget \
    xz-utils \
    && rm -rf /var/lib/apt/lists/*

# 设置工作目录
WORKDIR /app

# 升级pip
RUN python3.11 -m pip install --upgrade pip

# 复制预下载的cuDNN并安装
COPY downloads/cudnn-linux-x86_64-9.1.0.70_cuda11-archive.tar.xz /tmp/
RUN cd /tmp \
    && tar -xf cudnn-linux-x86_64-9.1.0.70_cuda11-archive.tar.xz \
    && cp cudnn-linux-x86_64-9.1.0.70_cuda11-archive/lib/* /usr/local/cuda/lib64/ \
    && cp cudnn-linux-x86_64-9.1.0.70_cuda11-archive/include/* /usr/local/cuda/include/ \
    && rm -rf /tmp/cudnn-linux-x86_64-9.1.0.70_cuda11-archive* \
    && echo "/usr/local/cuda/lib64" >> /etc/ld.so.conf.d/cuda.conf \
    && ldconfig

# 复制并安装预下载的PyTorch wheels (CUDA 11.8版本)
COPY downloads/torch_wheels/ /tmp/torch_wheels/
RUN python3.11 -m pip install /tmp/torch_wheels/*.whl --no-deps \
    && rm -rf /tmp/torch_wheels

# 复制并安装预下载的Python依赖wheels  
COPY downloads/python_wheels/ /tmp/python_wheels/
RUN python3.11 -m pip install /tmp/python_wheels/*.whl --no-deps \
    && rm -rf /tmp/python_wheels

# 安装任何遗漏的依赖（应该很少）
COPY requirements.txt .
RUN python3.11 -m pip install -r requirements.txt --no-cache-dir

# 清理pip缓存避免重复占用空间
RUN rm -rf /root/.cache/pip/*

# 复制应用代码
COPY backend/ ./backend/

# 创建必要的目录
RUN mkdir -p /app/data/uploads \
             /app/data/videos \
             /app/data/audios \
             /app/data/thumbnails \
             /app/data/local-videos \
             /app/logs \
             ./local-videos

# 设置模型缓存环境变量
ENV HF_HOME=/root/.cache/huggingface
ENV TRANSFORMERS_CACHE=/root/.cache/huggingface

# 创建模型缓存目录并复制预下载的模型
RUN mkdir -p /root/.cache/huggingface/hub
COPY models/faster-whisper-large-v3 /root/.cache/huggingface/hub/models--Systran--faster-whisper-large-v3

# 设置权限
RUN chmod -R 755 /app

# 环境变量
ENV PYTHONPATH=/app/backend
ENV ENVIRONMENT=production
ENV WHISPER_DEVICE=cuda
ENV WHISPER_COMPUTE_TYPE=float16
ENV WHISPER_MODEL=large
ENV MAX_CONCURRENT_TRANSCRIPTIONS=3
ENV FORCE_CPU_MODE=false

# 健康检查
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# 暴露端口
EXPOSE 8000

# 启动命令
CMD ["python3.11", "-m", "uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]