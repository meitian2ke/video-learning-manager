#!/bin/bash

# ä¿®å¤Whisperæ¨¡å‹å’Œé…ç½®é—®é¢˜

set -e

echo "ğŸ”§ ä¿®å¤Whisperæ¨¡å‹å’ŒGPUé…ç½®..."

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

print_status() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# 1. åœæ­¢ç°æœ‰æœåŠ¡
print_status "åœæ­¢ç°æœ‰æœåŠ¡..."
docker-compose -f docker-compose.gpu.yml down

# 2. åˆ›å»ºæ¨¡å‹ä¸‹è½½è„šæœ¬
print_status "åˆ›å»ºWhisperæ¨¡å‹é¢„ä¸‹è½½è„šæœ¬..."
cat > download_whisper_model.py << 'EOF'
#!/usr/bin/env python3
import os
import sys

def download_whisper_model():
    """é¢„ä¸‹è½½Whisperæ¨¡å‹"""
    print("ğŸ” å¼€å§‹ä¸‹è½½Whisperæ¨¡å‹...")
    
    try:
        # è®¾ç½®HuggingFaceç¼“å­˜ç›®å½•
        os.environ['HF_HOME'] = '/root/.cache/huggingface'
        os.environ['TRANSFORMERS_CACHE'] = '/root/.cache/huggingface'
        
        # æ£€æŸ¥æ˜¯å¦æœ‰GPU
        try:
            import torch
            gpu_available = torch.cuda.is_available()
            if gpu_available:
                device = "cuda"
                compute_type = "float16"
                print(f"âœ… GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
            else:
                device = "cpu" 
                compute_type = "int8"
                print("âš ï¸ GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨¡å¼")
        except ImportError:
            device = "cpu"
            compute_type = "int8"
            print("âš ï¸ PyTorchæœªæ‰¾åˆ°ï¼Œä½¿ç”¨CPUæ¨¡å¼")
        
        print(f"ğŸ“¥ ä¸‹è½½Whisper baseæ¨¡å‹åˆ° {device}...")
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        cache_dir = "/root/.cache/huggingface/hub"
        os.makedirs(cache_dir, exist_ok=True)
        
        # å¯¼å…¥faster_whisperå¹¶ä¸‹è½½æ¨¡å‹
        from faster_whisper import WhisperModel
        
        # ä¸‹è½½æ¨¡å‹ï¼ˆè¿™ä¼šè‡ªåŠ¨ä¸‹è½½åˆ°HuggingFaceç¼“å­˜ï¼‰
        print("ğŸ“¥ æ­£åœ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶...")
        model = WhisperModel("base", device=device, compute_type=compute_type, download_root=cache_dir)
        print("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ!")
        
        # ç®€å•æµ‹è¯•ï¼ˆä¸éœ€è¦å®é™…éŸ³é¢‘æ–‡ä»¶ï¼‰
        print("ğŸ§ª æ¨¡å‹åŠ è½½æµ‹è¯•å®Œæˆ")
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        model_path = "/root/.cache/huggingface/hub/models--guillaumekln--faster-whisper-base"
        if os.path.exists(model_path):
            print(f"âœ… æ¨¡å‹æ–‡ä»¶ç¡®è®¤å­˜åœ¨: {model_path}")
        else:
            print(f"âš ï¸ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            # åˆ—å‡ºå®é™…çš„ç¼“å­˜ç›®å½•å†…å®¹
            print("ğŸ“‚ å®é™…ç¼“å­˜ç›®å½•å†…å®¹:")
            if os.path.exists("/root/.cache/huggingface/hub"):
                for item in os.listdir("/root/.cache/huggingface/hub"):
                    print(f"  - {item}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = download_whisper_model()
    exit(0 if success else 1)
EOF

# 3. æ›´æ–°Dockerfileé¢„ä¸‹è½½æ¨¡å‹
print_status "æ›´æ–°GPU Dockerfileæ·»åŠ æ¨¡å‹é¢„ä¸‹è½½..."

# å¤‡ä»½åŸæ–‡ä»¶
cp Dockerfile.gpu Dockerfile.gpu.backup

# åœ¨Dockerfileä¸­æ·»åŠ æ¨¡å‹ä¸‹è½½æ­¥éª¤
cat > Dockerfile.gpu << 'EOF'
# GPUä¼˜åŒ–ç‰ˆDockerfile - ä¸“ä¸ºç”Ÿäº§ç¯å¢ƒè®¾è®¡
FROM nvidia/cuda:11.8.0-devel-ubuntu22.04

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# è®¾ç½®HuggingFaceç¼“å­˜ç›®å½•
ENV HF_HOME=/root/.cache/huggingface
ENV TRANSFORMERS_CACHE=/root/.cache/huggingface
ENV HF_HUB_CACHE=/root/.cache/huggingface/hub

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3.11-dev \
    python3-pip \
    ffmpeg \
    git \
    curl \
    wget \
    && rm -rf /var/lib/apt/lists/*

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å‡çº§pipï¼ˆç›´æ¥ä½¿ç”¨python3.11ï¼‰
RUN python3.11 -m pip install --upgrade pip

# å¤åˆ¶requirementsæ–‡ä»¶
COPY requirements.txt .

# é…ç½®pipä½¿ç”¨å›½å†…é•œåƒæºï¼ˆå¤§å¹…æå‡ä¸‹è½½é€Ÿåº¦ï¼‰
RUN python3.11 -m pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple/ && \
    python3.11 -m pip config set global.trusted-host pypi.tuna.tsinghua.edu.cn

# å®‰è£…PyTorch GPUç‰ˆæœ¬ï¼ˆä½¿ç”¨å›½å†…æº + CUDA 11.8ï¼‰
RUN python3.11 -m pip install torch torchvision torchaudio \
    -i https://pypi.tuna.tsinghua.edu.cn/simple/ \
    --extra-index-url https://download.pytorch.org/whl/cu118

# å®‰è£…å…¶ä»–ä¾èµ–
RUN python3.11 -m pip install --no-cache-dir -r requirements.txt

# åˆ›å»ºç¼“å­˜ç›®å½•
RUN mkdir -p /root/.cache/huggingface/hub

# å¤åˆ¶æ¨¡å‹ä¸‹è½½è„šæœ¬
COPY download_whisper_model.py .

# é¢„ä¸‹è½½Whisperæ¨¡å‹ï¼ˆå…³é”®æ­¥éª¤ï¼ï¼‰
RUN echo "ğŸ”¥ å¼€å§‹é¢„ä¸‹è½½Whisperæ¨¡å‹..." && \
    python3.11 download_whisper_model.py && \
    echo "âœ… Whisperæ¨¡å‹é¢„ä¸‹è½½å®Œæˆ" && \
    ls -la /root/.cache/huggingface/hub/ || echo "æ¨¡å‹ç›®å½•æ£€æŸ¥å¤±è´¥"

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY backend/ ./backend/
COPY local-videos/ ./local-videos/

# åˆ›å»ºå¿…è¦çš„ç›®å½•
RUN mkdir -p /app/data/uploads \
             /app/data/videos \
             /app/data/audios \
             /app/data/thumbnails \
             /app/data/local-videos \
             /app/logs

# è®¾ç½®æƒé™
RUN chmod -R 755 /app

# ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app/backend
ENV ENVIRONMENT=production
ENV WHISPER_DEVICE=cuda
ENV WHISPER_COMPUTE_TYPE=float16
ENV WHISPER_MODEL=base
ENV MAX_CONCURRENT_TRANSCRIPTIONS=3
ENV FORCE_CPU_MODE=false
ENV TRANSCRIPTION_MODE=local

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python3.11", "-m", "uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]
EOF

print_success "Dockerfileå·²æ›´æ–°ï¼Œæ·»åŠ äº†æ¨¡å‹é¢„ä¸‹è½½"

# 4. æ£€æŸ¥é…ç½®æ–‡ä»¶
print_status "æ£€æŸ¥åç«¯é…ç½®..."
if grep -q "TRANSCRIPTION_MODE.*openai" backend/app/core/config.py; then
    print_warning "å‘ç°é…ç½®ä¸­è¿˜æœ‰OpenAIæ¨¡å¼ï¼Œæ­£åœ¨ä¿®æ”¹ä¸ºæœ¬åœ°æ¨¡å¼..."
    
    # å¤‡ä»½é…ç½®æ–‡ä»¶
    cp backend/app/core/config.py backend/app/core/config.py.backup
    
    # ä¿®æ”¹é…ç½®ä¸ºæœ¬åœ°æ¨¡å¼
    sed -i 's/TRANSCRIPTION_MODE.*=.*"openai"/TRANSCRIPTION_MODE: str = "local"/' backend/app/core/config.py
    sed -i 's/TRANSCRIPTION_MODE.*=.*"auto"/TRANSCRIPTION_MODE: str = "local"/' backend/app/core/config.py
    
    print_success "é…ç½®å·²ä¿®æ”¹ä¸ºæœ¬åœ°æ¨¡å¼"
else
    print_success "é…ç½®å·²ç»æ˜¯æœ¬åœ°æ¨¡å¼"
fi

# 5. é‡æ–°æ„å»ºé•œåƒ
print_status "é‡æ–°æ„å»ºGPUé•œåƒï¼ˆåŒ…å«æ¨¡å‹é¢„ä¸‹è½½ï¼‰..."
docker-compose -f docker-compose.gpu.yml build --no-cache

# 6. å¯åŠ¨æœåŠ¡
print_status "å¯åŠ¨ä¿®å¤åçš„æœåŠ¡..."
docker-compose -f docker-compose.gpu.yml up -d

# 7. ç­‰å¾…æœåŠ¡å¯åŠ¨
print_status "ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 15

# 8. æ£€æŸ¥æœåŠ¡çŠ¶æ€
if curl -s http://localhost/health > /dev/null 2>&1; then
    print_success "âœ… æœåŠ¡å¯åŠ¨æˆåŠŸ!"
    echo ""
    echo "ğŸ“Š æœåŠ¡ä¿¡æ¯:"
    echo "  - å‰ç«¯ç•Œé¢: http://$(hostname -I | awk '{print $1}')"
    echo "  - GPUç›‘æ§: http://$(hostname -I | awk '{print $1}')/api/gpu/status"
    echo ""
    echo "ğŸ”§ æ•…éšœæ’é™¤:"
    echo "  - æŸ¥çœ‹æ—¥å¿—: docker-compose -f docker-compose.gpu.yml logs -f"
    echo "  - æ£€æŸ¥GPU: nvidia-smi"
    echo "  - æµ‹è¯•è½¬å½•: ä¸Šä¼ ä¸€ä¸ªå°è§†é¢‘æ–‡ä»¶"
else
    print_error "âŒ æœåŠ¡å¯åŠ¨å¤±è´¥!"
    echo ""
    echo "ğŸ“‹ æ£€æŸ¥æ­¥éª¤:"
    echo "1. æŸ¥çœ‹å®¹å™¨çŠ¶æ€: docker-compose -f docker-compose.gpu.yml ps"
    echo "2. æŸ¥çœ‹åç«¯æ—¥å¿—: docker logs video-learning-manager-gpu"
    echo "3. æŸ¥çœ‹å‰ç«¯æ—¥å¿—: docker logs video-learning-frontend"
fi

print_success "ä¿®å¤è„šæœ¬æ‰§è¡Œå®Œæˆ!"