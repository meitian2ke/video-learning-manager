#!/bin/bash

# ğŸ¤– Whisperæ¨¡å‹ä¸‹è½½è„šæœ¬
# ç¡®ä¿æ‰€æœ‰å¿…è¦çš„æ¨¡å‹éƒ½å·²ä¸‹è½½

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

print_status() { echo -e "${BLUE}[INFO]${NC} $1"; }
print_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
print_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
print_error() { echo -e "${RED}[ERROR]${NC} $1"; }

echo "ğŸ¤– Whisperæ¨¡å‹ä¸‹è½½è„šæœ¬"
echo "==============================================="

# æ£€æŸ¥å®¹å™¨æ˜¯å¦è¿è¡Œ
if ! docker ps | grep -q video-learning-manager-gpu; then
    print_error "åç«¯å®¹å™¨æœªè¿è¡Œï¼è¯·å…ˆå¯åŠ¨æœåŠ¡"
    print_status "è¿è¡Œ: docker-compose -f docker-compose.gpu.yml up -d"
    exit 1
fi

print_success "æ‰¾åˆ°è¿è¡Œä¸­çš„å®¹å™¨"

# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
print_status "æ£€æŸ¥ç°æœ‰æ¨¡å‹..."
if docker exec video-learning-manager-gpu test -d /root/.cache/huggingface/hub/models--guillaumekln--faster-whisper-base; then
    print_success "âœ… Whisper baseæ¨¡å‹å·²å­˜åœ¨"
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    model_size=$(docker exec video-learning-manager-gpu du -sh /root/.cache/huggingface/hub/models--guillaumekln--faster-whisper-base | cut -f1)
    print_status "æ¨¡å‹å¤§å°: $model_size"
    
    # æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ç”¨
    print_status "æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ç”¨..."
    if docker exec video-learning-manager-gpu python3.11 -c "
from faster_whisper import WhisperModel
try:
    model = WhisperModel('base', device='cuda', compute_type='float16')
    print('âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨')
except Exception as e:
    print(f'âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}')
    exit(1)
" 2>/dev/null; then
        print_success "ğŸš€ æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼Œå¯ä»¥å¼€å§‹å¤„ç†è§†é¢‘äº†ï¼"
        exit 0
    else
        print_warning "æ¨¡å‹å­˜åœ¨ä½†æ— æ³•æ­£å¸¸åŠ è½½ï¼Œé‡æ–°ä¸‹è½½..."
    fi
else
    print_warning "âŒ æ¨¡å‹æœªæ‰¾åˆ°ï¼Œå¼€å§‹ä¸‹è½½..."
fi

# ä¸‹è½½æ¨¡å‹
print_status "å¼€å§‹ä¸‹è½½Whisperæ¨¡å‹ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰..."

# åˆ›å»ºä¸‹è½½è„šæœ¬
cat > /tmp/download_whisper.py << 'EOF'
import os
import sys
from faster_whisper import WhisperModel

def download_model():
    """ä¸‹è½½Whisperæ¨¡å‹"""
    try:
        print("ğŸ” è®¾ç½®ç¼“å­˜ç›®å½•...")
        os.environ['HF_HOME'] = '/root/.cache/huggingface'
        os.environ['TRANSFORMERS_CACHE'] = '/root/.cache/huggingface'
        
        print("ğŸ“¥ å¼€å§‹ä¸‹è½½Whisper baseæ¨¡å‹...")
        print("è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        # ä¸‹è½½æ¨¡å‹
        model = WhisperModel('base', device='cuda', compute_type='float16')
        
        print("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
        
        # éªŒè¯æ¨¡å‹
        print("ğŸ§ª éªŒè¯æ¨¡å‹...")
        model_path = '/root/.cache/huggingface/hub/models--guillaumekln--faster-whisper-base'
        if os.path.exists(model_path):
            print(f"âœ… æ¨¡å‹æ–‡ä»¶ç¡®è®¤å­˜åœ¨: {model_path}")
            
            # æ˜¾ç¤ºæ¨¡å‹æ–‡ä»¶
            for root, dirs, files in os.walk(model_path):
                for file in files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶
                    print(f"  - {file}")
            
            return True
        else:
            print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return False
            
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = download_model()
    sys.exit(0 if success else 1)
EOF

# å¤åˆ¶è„šæœ¬åˆ°å®¹å™¨å¹¶æ‰§è¡Œ
docker cp /tmp/download_whisper.py video-learning-manager-gpu:/tmp/

print_status "åœ¨å®¹å™¨å†…æ‰§è¡Œæ¨¡å‹ä¸‹è½½..."
if docker exec video-learning-manager-gpu python3.11 /tmp/download_whisper.py; then
    print_success "ğŸ‰ Whisperæ¨¡å‹ä¸‹è½½å®Œæˆï¼"
    
    # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€
    echo ""
    print_status "ğŸ“Š æ¨¡å‹çŠ¶æ€æ€»ç»“:"
    docker exec video-learning-manager-gpu ls -la /root/.cache/huggingface/hub/ | grep whisper || echo "  æ— whisperç›¸å…³æ–‡ä»¶"
    
    # æœ€ç»ˆæµ‹è¯•
    print_status "ğŸ§ª æœ€ç»ˆåŠŸèƒ½æµ‹è¯•..."
    if docker exec video-learning-manager-gpu python3.11 -c "
from faster_whisper import WhisperModel
model = WhisperModel('base', device='cuda', compute_type='float16')
print('ğŸš€ æ¨¡å‹å¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼ŒGPUè½¬å½•åŠŸèƒ½å·²å°±ç»ªï¼')
" 2>/dev/null; then
        print_success "âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œå¯ä»¥å¼€å§‹å¤„ç†è§†é¢‘äº†ï¼"
        echo ""
        print_status "ğŸ’¡ æç¤º: ç°åœ¨å¯ä»¥åœ¨Webç•Œé¢ä¸Šä¼ è§†é¢‘è¿›è¡ŒGPUåŠ é€Ÿè½¬å½•"
    else
        print_error "âŒ æœ€ç»ˆæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥GPUç¯å¢ƒ"
        exit 1
    fi
else
    print_error "âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥"
    echo ""
    print_status "ğŸ”§ æ•…éšœæ’é™¤:"
    echo "1. æ£€æŸ¥ç½‘ç»œè¿æ¥"
    echo "2. æ£€æŸ¥å®¹å™¨å†…å­˜æ˜¯å¦å……è¶³"  
    echo "3. æ£€æŸ¥GPUé©±åŠ¨æ˜¯å¦æ­£å¸¸"
    echo "4. é‡å¯å®¹å™¨: docker restart video-learning-manager-gpu"
    exit 1
fi

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -f /tmp/download_whisper.py
docker exec video-learning-manager-gpu rm -f /tmp/download_whisper.py

echo ""
print_success "ğŸ¯ æ¨¡å‹ä¸‹è½½å’ŒéªŒè¯å®Œæˆï¼ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªã€‚"